{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c9744b5-3f46-4eae-aa96-c9a761630e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Install Spark + import lib + start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61af8be2-82ca-4a5a-b355-87a70ad0d121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "bKiT8ZFQ9Sqs"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef354bb6-ff60-47ed-9725-a7feb6064b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "iCw_wOW_1843"
   },
   "source": [
    "# Getting data from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56c277b5-d090-4cb3-879c-5f2ae8396a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "MrIZV6G52HWp"
   },
   "source": [
    "You may download data from Kaggle mannually or using auto pipeline like this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6d0860-edfb-454e-943d-b90f13704c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install opendatasets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8507155-22fb-4b52-9f85-4e10247f98fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/claudiodavi/superhero-set/data\",\"/Volumes/workspace/default/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7b8691-3890-4fa6-8f45-92eddeab0a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c5efe0-f9fc-4acd-bef0-a40615aa381d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "KClq7t6OhgDL",
    "outputId": "43516475-92b7-4dd4-c8c0-1e144ce0171f"
   },
   "outputs": [],
   "source": [
    "%sh unzip /dbfs/FileStore/mypath/superhero-set.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bed9378-154b-44ae-8940-abd351c9a277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxKcQZcoh9i2",
    "outputId": "4e505c93-7b19-42c6-f0ed-ba31fa760651"
   },
   "outputs": [],
   "source": [
    "#important \"file:/\"\n",
    "\n",
    "file_path = \"/Volumes/workspace/default/test/superhero-set/heroes_information.csv\"\n",
    "\n",
    "\n",
    "df_hero_indi = spark.read.options(header=\"true\",inferschema = \"true\").csv(file_path)\n",
    "\n",
    "df_hero_indi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa0b8577-2bf0-4a7f-afc0-7d2b41ae5114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_hero_indi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01c6e381-bd5b-436c-b78f-043c07ddb0a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cplaIPgVi5Bo",
    "outputId": "9a8f5ba0-8808-46c1-ef52-c6da9820d918"
   },
   "outputs": [],
   "source": [
    "file_path2 = \"/Volumes/workspace/default/test/superhero-set/super_hero_powers.csv\"\n",
    "\n",
    "df_hero_power = spark.read.options(header=\"true\",inferschema = \"true\").csv(file_path2)\n",
    "\n",
    "#df_hero_power.show()\n",
    "display(df_hero_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7744ae1f-5d4a-4062-bc1a-1f72e785be4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "DckC1xuX8x5R"
   },
   "source": [
    "# Querying + Stat test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528e8255-fda5-4321-82bd-4681a9e3e1f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKJK3X2LjNZ9",
    "outputId": "a83bae14-843a-460f-ffa0-42cf5bbb127a"
   },
   "outputs": [],
   "source": [
    "df_hero_indi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ff0b3b-71c5-42a6-9b8e-314f05c2e36c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYI6IXHmKLEF",
    "outputId": "4015b04f-8052-4929-f8ef-4f7f72b55ae2"
   },
   "outputs": [],
   "source": [
    "display(df_hero_indi.select(\"Race\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffd6c23e-abc6-4a9d-8d6c-5640c4c0c8d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spURyBYTKTnv",
    "outputId": "e25f5dca-217c-4c86-a41d-ba35e505d2e2"
   },
   "outputs": [],
   "source": [
    "df_hero_indi.select(\"Race\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4622a4-5cc3-4020-a8b2-d0acfcb7a45e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5l8lQFiwKyHN",
    "outputId": "b6c3260a-ddff-43f5-be8b-015617c41f0a"
   },
   "outputs": [],
   "source": [
    "display(df_hero_indi.filter(col(\"Race\")==\"Cyborg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3064221-7133-440d-aacf-f1bc51c7208d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cFaL8AUj8w3",
    "outputId": "646aee1e-acbe-4860-a13d-ca4e2fd5a1c1"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "df_hero_indi.agg(countDistinct(col(\"Race\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b951bdf-505f-4f25-a825-dd8bb78070c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwywxoCAjWZo",
    "outputId": "4f70516c-fdb2-4a14-fcf1-162bb7f09791"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct\n",
    "\n",
    "display(df_hero_indi.agg(*(countDistinct(col(c)).alias(c) for c in df_hero_indi.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75bb5527-2913-4967-9eda-2b472f413832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12qsEZZXne3-",
    "outputId": "d4d8e430-0945-41b6-8934-901e2a2d482a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_hero_indi.agg(F.min(col(\"Weight\"))\\\n",
    "              ,F.max(col(\"Weight\"))\\\n",
    "              ,F.avg(col(\"Weight\"))\\\n",
    "              ,F.sum(col(\"Weight\"))\\\n",
    "              ,F.stddev(col(\"Weight\")))\\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a579f2e5-5086-4b14-a367-f273ea45e59c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Iqcv9oW-HiV",
    "outputId": "cc818a1c-dc0d-42d2-d7c9-96c5dd6ba448"
   },
   "outputs": [],
   "source": [
    "display(\n",
    "df_hero_indi.groupBy(col(\"Race\")).agg(F.min(col(\"Weight\"))\\\n",
    "              ,F.max(col(\"Weight\"))\\\n",
    "              ,F.avg(col(\"Weight\"))\\\n",
    "              ,F.sum(col(\"Weight\"))\\\n",
    "              ,F.stddev(col(\"Weight\")))\\\n",
    ")\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df775ae4-41d1-47f4-a7d4-79e369134a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1d4ezAuyfxHM"
   },
   "source": [
    "## Finding Median\n",
    "http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3f88cf-bef5-4b51-a67e-7b036bc06cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B02ZhiwSfcuF",
    "outputId": "61717f36-66ba-479a-f401-e9ce14846d5f"
   },
   "outputs": [],
   "source": [
    "df_hero_indi.approxQuantile(\"weight\", [0.5], 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4824391a-32ea-4598-889d-9e3b370934db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kOc52NplFKmm"
   },
   "source": [
    "## \"Null\" **checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fb98ed7-148e-4115-8afc-e2d87664cae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZKI0Dj7eFQwG",
    "outputId": "d820b20f-3702-4a46-a0c5-68b605ecff15"
   },
   "outputs": [],
   "source": [
    "df_hero_indi.filter(col(\"Weight\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8805b43a-4e39-4dc1-911f-9a33bd6b5792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "7wXp5oecQaO2"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53b37e61-b1a6-41d0-9093-8dcc51ce06be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "19H0zTUl9mmS"
   },
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fbdc494-47a3-4c59-8da7-e3522092dec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i0hzZzQKGHBa",
    "outputId": "e98f5de8-98a2-4698-c6f0-b30488881a84"
   },
   "outputs": [],
   "source": [
    "df_hero_indi.groupBy(\"Race\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85257d53-df2a-423e-8330-90ab1ce9e5fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwipUu3z9l1C",
    "outputId": "4c958c73-d0c4-4e6a-fa13-3473214a038e"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_hero_indi.groupby(col(\"Gender\")).agg(F.min(col(\"Weight\"))\\\n",
    "              ,F.max(col(\"Weight\"))\\\n",
    "              ,F.avg(col(\"Weight\"))\\\n",
    "              ,F.sum(col(\"Weight\"))\\\n",
    "              ,F.stddev(col(\"Weight\")))\\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d7b33e-4db8-4d5e-b071-edcb90b116e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2lxGRWDqIWH",
    "outputId": "277a0fcd-b563-4b7c-f8c7-070a0843c7de"
   },
   "outputs": [],
   "source": [
    "df_hero_weight = df_hero_indi.filter(col(\"Weight\")!=-99).select(\"weight\")\n",
    "df_hero_weight.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b75b9f-2100-4755-8f94-a5b7b0bd9312",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OW8IvINYqgoG",
    "outputId": "686f145e-246b-4210-e14f-2b27422fcc55"
   },
   "outputs": [],
   "source": [
    "df_hero_weight.agg(F.min(col(\"Weight\"))\\\n",
    "              ,F.max(col(\"Weight\"))\\\n",
    "              ,F.avg(col(\"Weight\"))\\\n",
    "              ,F.sum(col(\"Weight\"))\\\n",
    "              ,F.stddev(col(\"Weight\")))\\\n",
    "              .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd85a552-ce1b-4b63-9bd8-87a049712433",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HAU2xxMarvHj",
    "outputId": "dd623c62-26e6-4a38-c7d6-27f00d60d075"
   },
   "outputs": [],
   "source": [
    "display(df_hero_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bf0ea98-bd12-4acb-81c9-feff47e8f7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4776691178928453>, line 11\u001B[0m\n",
       "\u001B[1;32m      7\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhistogram\u001B[39m\u001B[38;5;124m'\u001B[39m: hist_counts\u001B[38;5;241m.\u001B[39mvalues})\n",
       "\u001B[1;32m      9\u001B[0m weight_histogram \u001B[38;5;241m=\u001B[39m df_hero_weight\u001B[38;5;241m.\u001B[39mmapInPandas(compute_histogram, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhistogram array<double>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 11\u001B[0m display(weight_histogram)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n",
       "\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n",
       "\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:107\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n",
       "\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:72\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n",
       "\u001B[0;32m---> 72\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n",
       "\u001B[1;32m     73\u001B[0m     ip_display({\n",
       "\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n",
       "\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     76\u001B[0m     },\n",
       "\u001B[1;32m     77\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:168\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n",
       "\u001B[1;32m    171\u001B[0m         e\n",
       "\u001B[1;32m    172\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    173\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:132\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n",
       "\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Check if the new cloudfetch API is available on the dataframe\u001B[39;00m\n",
       "\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39muse_pyspark_cloudfetch_api \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\n",
       "\u001B[1;32m    130\u001B[0m         connectDataFrame, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_to_cloudfetch_with_limits_and_file_paths\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[1;32m    131\u001B[0m     results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m], List[\n",
       "\u001B[0;32m--> 132\u001B[0m         \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n",
       "\u001B[1;32m    133\u001B[0m             \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n",
       "\u001B[1;32m    134\u001B[0m             compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n",
       "\u001B[1;32m    135\u001B[0m             row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n",
       "\u001B[1;32m    136\u001B[0m             byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n",
       "\u001B[1;32m    137\u001B[0m     pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n",
       "\u001B[1;32m    139\u001B[0m     schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(\n",
       "\u001B[1;32m    140\u001B[0m         pyspark_struct)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1963\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n",
       "\u001B[1;32m   1942\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1943\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n",
       "\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1960\u001B[0m \n",
       "\u001B[1;32m   1961\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1962\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n",
       "\u001B[0;32m-> 1963\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n",
       "\u001B[1;32m   1964\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   1965\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1083\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n",
       "\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n",
       "\u001B[1;32m   1081\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n",
       "\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1083\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n",
       "\u001B[1;32m   1084\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1085\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m   2146\u001B[0m                 )\n",
       "\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2150\u001B[0m                 info,\n",
       "\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mPythonException\u001B[0m: \n",
       "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
       "Traceback (most recent call last):\n",
       "  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/series.py\", line 4771, in apply\n",
       "    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1123, in apply\n",
       "    return self.apply_standard()\n",
       "           ^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1174, in apply_standard\n",
       "    mapped = lib.map_infer(\n",
       "             ^^^^^^^^^^^^^^\n",
       "  File \"pandas/_libs/lib.pyx\", line 2924, in pandas._libs.lib.map_infer\n",
       "  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1373, in <lambda>\n",
       "    lambda x: conv(x) if x is not None else None\n",
       "              ^^^^^^^\n",
       "  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1189, in convert_array\n",
       "    return list(value)\n",
       "           ^^^^^^^^^^^\n",
       "TypeError: 'int' object is not iterable\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "PythonException",
        "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/series.py\", line 4771, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1123, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1174, in apply_standard\n    mapped = lib.map_infer(\n             ^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2924, in pandas._libs.lib.map_infer\n  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1373, in <lambda>\n    lambda x: conv(x) if x is not None else None\n              ^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1189, in convert_array\n    return list(value)\n           ^^^^^^^^^^^\nTypeError: 'int' object is not iterable\n"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": null,
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPythonException\u001B[0m                           Traceback (most recent call last)",
        "File \u001B[0;32m<command-4776691178928453>, line 11\u001B[0m\n\u001B[1;32m      7\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhistogram\u001B[39m\u001B[38;5;124m'\u001B[39m: hist_counts\u001B[38;5;241m.\u001B[39mvalues})\n\u001B[1;32m      9\u001B[0m weight_histogram \u001B[38;5;241m=\u001B[39m df_hero_weight\u001B[38;5;241m.\u001B[39mmapInPandas(compute_histogram, schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhistogram array<double>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m display(weight_histogram)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001B[0m, in \u001B[0;36mDisplay.display\u001B[0;34m(self, input, *args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspark_connect_enabled \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdisplay_connect_table(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, ConnectDataFrame):\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39misStreaming:\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display.py:107\u001B[0m, in \u001B[0;36mDisplay.display_connect_table\u001B[0;34m(self, df, **kwargs)\u001B[0m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_streaming_dataframe(df, config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstreaming_listener,\n\u001B[1;32m    105\u001B[0m                                                \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 107\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcf_helper\u001B[38;5;241m.\u001B[39mdisplay_dataframe(df, config)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:72\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.display_dataframe\u001B[0;34m(self, df, config)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_dataframe\u001B[39m(\u001B[38;5;28mself\u001B[39m, df: ConnectDataFrame, config: TableDisplayConfig):\n\u001B[0;32m---> 72\u001B[0m     display_payload \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_to_cloudfetch(df, config)\n\u001B[1;32m     73\u001B[0m     ip_display({\n\u001B[1;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mapplication/vnd.databricks.connect.display\u001B[39m\u001B[38;5;124m\"\u001B[39m: display_payload,\n\u001B[1;32m     75\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext/plain\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatabricks Spark Connect Table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     76\u001B[0m     },\n\u001B[1;32m     77\u001B[0m                raw\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:168\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m PySparkException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 168\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    171\u001B[0m         e\n\u001B[1;32m    172\u001B[0m     )(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEncountered an unexpected error when displaying table, please contact Databricks support\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    173\u001B[0m       ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/display_helpers/cloudfetch_helper.py:132\u001B[0m, in \u001B[0;36mCloudFetchDisplayHelper.write_to_cloudfetch\u001B[0;34m(self, connectDataFrame, config)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# Check if the new cloudfetch API is available on the dataframe\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39muse_pyspark_cloudfetch_api \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\n\u001B[1;32m    130\u001B[0m         connectDataFrame, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_to_cloudfetch_with_limits_and_file_paths\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    131\u001B[0m     results: Tuple[Optional[StructType], List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCloudFetchResult\u001B[39m\u001B[38;5;124m\"\u001B[39m], List[\n\u001B[0;32m--> 132\u001B[0m         \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m connectDataFrame\u001B[38;5;241m.\u001B[39m_to_cloudfetch_with_limits_and_file_paths(\n\u001B[1;32m    133\u001B[0m             \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mstr_format,\n\u001B[1;32m    134\u001B[0m             compression\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    135\u001B[0m             row_limit\u001B[38;5;241m=\u001B[39mrowLimit,\n\u001B[1;32m    136\u001B[0m             byte_limit\u001B[38;5;241m=\u001B[39mbyteLimit)\n\u001B[1;32m    137\u001B[0m     pyspark_struct, cloudfetch_results, batch_truncation_results \u001B[38;5;241m=\u001B[39m results\n\u001B[1;32m    139\u001B[0m     schema \u001B[38;5;241m=\u001B[39m CloudFetchDisplayHelper\u001B[38;5;241m.\u001B[39mget_display_schema_from_pyspark_struct(\n\u001B[1;32m    140\u001B[0m         pyspark_struct)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:1963\u001B[0m, in \u001B[0;36mDataFrame._to_cloudfetch_with_limits_and_file_paths\u001B[0;34m(self, format, compression, row_limit, byte_limit)\u001B[0m\n\u001B[1;32m   1942\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1943\u001B[0m \u001B[38;5;124;03mThis is an experimental method to support generating results in the form of pre-signed\u001B[39;00m\n\u001B[1;32m   1944\u001B[0m \u001B[38;5;124;03mURLs that can be used by the client to fetch the results directly from cloud storage.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1960\u001B[0m \n\u001B[1;32m   1961\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1962\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_plan\u001B[38;5;241m.\u001B[39mto_proto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient)\n\u001B[0;32m-> 1963\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_session\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexperimental_to_cloudfetch(\n\u001B[1;32m   1964\u001B[0m     query, \u001B[38;5;28mformat\u001B[39m, compression, row_limit, byte_limit, \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1965\u001B[0m )\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1083\u001B[0m, in \u001B[0;36mSparkConnectClient.experimental_to_cloudfetch\u001B[0;34m(self, plan, format, compression, row_limit, byte_limit, with_internal_file_path)\u001B[0m\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;66;03m# Execute the request and iterate over the streaming results. We capture the schema\u001B[39;00m\n\u001B[1;32m   1081\u001B[0m \u001B[38;5;66;03m# response and unpack the CloudResultBatch responses.\u001B[39;00m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1083\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(req, {}, [], progress\u001B[38;5;241m=\u001B[39mprogress):\n\u001B[1;32m   1084\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1085\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1731\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1729\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1730\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1731\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2047\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2045\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2046\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2047\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2048\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2049\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2149\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2134\u001B[0m                 \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m   2135\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPython versions in the Spark Connect client and server are different. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2136\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo execute user-defined functions, client and server should have the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://docs.databricks.com/en/release-notes/serverless.html.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2146\u001B[0m                 )\n\u001B[1;32m   2147\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2149\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2150\u001B[0m                 info,\n\u001B[1;32m   2151\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2152\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2153\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2154\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2156\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2157\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mPythonException\u001B[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/series.py\", line 4771, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1123, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/pandas/core/apply.py\", line 1174, in apply_standard\n    mapped = lib.map_infer(\n             ^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2924, in pandas._libs.lib.map_infer\n  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1373, in <lambda>\n    lambda x: conv(x) if x is not None else None\n              ^^^^^^^\n  File \"/databricks/spark/python/pyspark/sql/pandas/types.py\", line 1189, in convert_array\n    return list(value)\n           ^^^^^^^^^^^\nTypeError: 'int' object is not iterable\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_histogram(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        hist, bin_edges = pd.cut(pdf['Weight'], bins=11, retbins=True)\n",
    "        hist_counts = hist.value_counts().sort_index()\n",
    "        yield pd.DataFrame({'histogram': hist_counts.values})\n",
    "\n",
    "weight_histogram = df_hero_weight.mapInPandas(compute_histogram, schema=\"histogram array<double>\")\n",
    "\n",
    "display(weight_histogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed722630-8488-4484-8692-a89cf894dc33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "n0NdtX8W-nuv"
   },
   "source": [
    "(Using Pandas and plot for showing graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c64bb66-2961-4a84-933c-0873e2e1a129",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "IELW9L-H-Rug"
   },
   "source": [
    "## Joinning (yes, same as join in SQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "786cd473-af3e-4acd-aa2b-87427e4b3e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oAoP3Xjuk2b8",
    "outputId": "04e3c502-5a91-4798-e911-0bb03176b630"
   },
   "outputs": [],
   "source": [
    "#Rename to match\n",
    "df_power = df_hero_power.withColumnRenamed(\"hero_names\",\"name\")\n",
    "\n",
    "df_power.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93c92968-dc5d-4ff8-9aca-0565834b6480",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CjD7e8K-kM_",
    "outputId": "c01cfd16-69de-40d2-eddd-f81f3ecb621a"
   },
   "outputs": [],
   "source": [
    "df_joined = df_hero_indi.join(df_power, on=\"name\",how=\"left\")\n",
    "display(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad443ebe-3217-4b06-85d6-d2a527ff6fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "EXaHFl9_gXfx"
   },
   "source": [
    "# Basic Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0829f04-7b95-47f6-a0da-563ae429e9e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "8eND1Kk-H7R-"
   },
   "source": [
    "## New conditional column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f68648-e04a-4582-bcc1-ca46008ab83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "Bs0LUgeYIJjN"
   },
   "source": [
    "Due to WORM (write once read many) so normally we will not alter df, we would add with new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6c3a44-a0c2-4bd1-85c4-8534b49096ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HR28Jg5TH6mP",
    "outputId": "fd813266-ac20-4d9f-bac2-3273bb07220b"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "new_column = F.when(col(\"Race\")==\"-\",\"null\").otherwise(col(\"Race\"))\n",
    "\n",
    "df_test_nc = df_hero_indi.withColumn(\"clean_Race\",new_column)\n",
    "df_test_nc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7dd9cca-8d06-468c-b43f-ea78886daaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "uiNUq462QN-H"
   },
   "source": [
    "Apply same concept to clean null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc64f96-9bbb-433d-84d5-68d6ef3d980b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHMJlH-5QSEn",
    "outputId": "817c2019-8aba-4f6d-8353-b0ecc09e162f"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when\n",
    "\n",
    "new_column = F.when(col(\"weight\").isNull(),-99).otherwise(col(\"weight\"))\n",
    "\n",
    "df_test_nc = df_hero_indi.withColumn(\"clean_weight1\",new_column)\n",
    "df_test_nc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bef565a-4be3-4689-bfcb-fc1e89146b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3Spu6hfogBFk"
   },
   "source": [
    "## UDF: User defined function(s)\n",
    "Spark does not support direct calculation to each cell values so there is some reway to do calculation, in distribution mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2f98af-f3db-4150-a56b-9c5241e0ba61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "6uvdQiE1g-t0",
    "outputId": "34baac96-4d68-41af-9f12-f5ba31365d32"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def lbs2kg(lbs):\n",
    "    return lbs*0.4536\n",
    "\n",
    "lbs2kg_udf = udf(lbs2kg, FloatType())\n",
    "df_test = df_hero_indi.withColumn('weight_in_kg',lbs2kg_udf(df_hero_indi[\"weight\"]))\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af535562-0e4e-484a-a022-037bbb38982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CCZJ0tqL8jp1"
   },
   "source": [
    "## Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fecdae35-1def-485e-9549-f67923c37a3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4AsqBsw7acZ",
    "outputId": "98c4c9c9-cbc1-47e8-9d62-b9758a60d98c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4776691178928408>, line 3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Binarizer\n",
       "\u001B[0;32m----> 3\u001B[0m binarizer \u001B[38;5;241m=\u001B[39m Binarizer(threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m112.25\u001B[39m, inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinarized_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m binarizedDataFrame \u001B[38;5;241m=\u001B[39m binarizer\u001B[38;5;241m.\u001B[39mtransform(df_hero_indi)\n",
       "\u001B[1;32m      5\u001B[0m binarizedDataFrame\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/feature.py:236\u001B[0m, in \u001B[0;36mBinarizer.__init__\u001B[0;34m(self, threshold, inputCol, outputCol, thresholds, inputCols, outputCols)\u001B[0m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m__init__(self, \\\\*, threshold=0.0, inputCol=None, outputCol=None, thresholds=None, \\\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m         inputCols=None, outputCols=None)\u001B[39;00m\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28msuper\u001B[39m(Binarizer, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
       "\u001B[0;32m--> 236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.feature.Binarizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid)\n",
       "\u001B[1;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setDefault(threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m)\n",
       "\u001B[1;32m    238\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/wrapper.py:90\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n",
       "\u001B[1;32m     88\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n",
       "\u001B[1;32m     89\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n",
       "\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_obj(\u001B[38;5;241m*\u001B[39mjava_args)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1620\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1614\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1616\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1617\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1619\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1620\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1621\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fqn)\n",
       "\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.ml.feature.Binarizer. Trace:\n",
       "py4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.Binarizer(java.lang.String) is not whitelisted.\n",
       "\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:256)\n",
       "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
       "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JError",
        "evalue": "An error occurred while calling None.org.apache.spark.ml.feature.Binarizer. Trace:\npy4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.Binarizer(java.lang.String) is not whitelisted.\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n\tat py4j.Gateway.invoke(Gateway.java:256)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-4776691178928408>, line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Binarizer\n\u001B[0;32m----> 3\u001B[0m binarizer \u001B[38;5;241m=\u001B[39m Binarizer(threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m112.25\u001B[39m, inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinarized_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m binarizedDataFrame \u001B[38;5;241m=\u001B[39m binarizer\u001B[38;5;241m.\u001B[39mtransform(df_hero_indi)\n\u001B[1;32m      5\u001B[0m binarizedDataFrame\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/feature.py:236\u001B[0m, in \u001B[0;36mBinarizer.__init__\u001B[0;34m(self, threshold, inputCol, outputCol, thresholds, inputCols, outputCols)\u001B[0m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;124;03m__init__(self, \\\\*, threshold=0.0, inputCol=None, outputCol=None, thresholds=None, \\\u001B[39;00m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;124;03m         inputCols=None, outputCols=None)\u001B[39;00m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28msuper\u001B[39m(Binarizer, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m--> 236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.feature.Binarizer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid)\n\u001B[1;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setDefault(threshold\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m)\n\u001B[1;32m    238\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/wrapper.py:90\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     88\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[1;32m     89\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_obj(\u001B[38;5;241m*\u001B[39mjava_args)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1620\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1614\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1616\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1617\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1619\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1620\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1621\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fqn)\n\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
        "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.ml.feature.Binarizer. Trace:\npy4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.Binarizer(java.lang.String) is not whitelisted.\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n\tat py4j.Gateway.invoke(Gateway.java:256)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=112.25, inputCol=\"Weight\", outputCol=\"binarized_weight\")\n",
    "binarizedDataFrame = binarizer.transform(df_hero_indi)\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "038671d7-56be-4634-aded-4d8dd9ad87d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "fg-gTociizgU"
   },
   "source": [
    "## Quatile / Percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f240e1a1-8c03-45d4-b433-85079c1cab3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4MA3-T4dkQu9",
    "outputId": "3aad2449-0f0e-4b2c-bee5-b35b8b1178c9"
   },
   "outputs": [],
   "source": [
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df_hero_weight.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in df_hero_weight.columns\n",
    "}\n",
    "\n",
    "print(bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f48e865-17c7-4467-96ff-a35e88f376d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "SYrDRiqeyE7e"
   },
   "source": [
    "Using quatile to check outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57872f04-c5f9-4851-aa8e-6440d3313f33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xH-EbI5Xxrkm",
    "outputId": "7077034f-689c-42e7-defa-1406b51e77b6"
   },
   "outputs": [],
   "source": [
    "for c in bounds:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "    bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)\n",
    "print(bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48da91e9-79f5-4fef-b8de-386da90a913b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwR6pRjExxXn",
    "outputId": "ccb906f2-04fb-475a-942b-e286290de958"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df_hero_weight.select(\n",
    "    \"*\",\n",
    "    *[\n",
    "        f.when(\n",
    "            f.col(c).between(bounds[c]['lower'], bounds[c]['upper']),\n",
    "            0\n",
    "        ).otherwise(1).alias(c+\"_out\") \n",
    "        for c in df_hero_weight.columns\n",
    "    ]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8197520d-d654-4ed5-a981-adc8f014bad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "r75YSs3A4oHP"
   },
   "source": [
    "Advanced solution for percentile / quatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5878a2e8-f737-48a0-bd2f-4465b312765f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "36t2cq094tXv"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import Column\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import ast\n",
    "\n",
    "\n",
    "class Discretize:\n",
    "    @staticmethod\n",
    "    def threshold_index(col_val, threshold: Column, threshold_str: bool = False):\n",
    "        if threshold_str:\n",
    "            # convert list that represent as string to normal list\n",
    "            threshold = ast.literal_eval(threshold)\n",
    "        for i, val_i in enumerate(threshold):\n",
    "            current = threshold[i]\n",
    "            if i > 0:\n",
    "                previous = threshold[i - 1]\n",
    "                if col_val > previous and col_val <= current:\n",
    "                    result = int(i)\n",
    "                elif col_val > previous and col_val > current:\n",
    "                    # for threshold cutoff (extend positive limit bound)\n",
    "                    result = int(i) + 1\n",
    "            if i == 0 and col_val <= current:\n",
    "                result = int(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def human_score(x, y):\n",
    "        return (int(y) - int(x))\n",
    "\n",
    "    @staticmethod\n",
    "    def indexer(df_in, columnname, x, output_name, invert: bool = True):\n",
    "\n",
    "        threshold_index_udf = udf(Discretize.threshold_index, IntegerType())\n",
    "        human_score_udf = udf(Discretize.human_score, IntegerType())\n",
    "\n",
    "        index = list(np.linspace(1. / x, 1, x))\n",
    "        pthvalue = statFunc(df_in).approxQuantile(columnname, index, 0.0)  # get list of cutoff nth //\n",
    "        df_out = df_in.withColumn(\"pth\", array([lit(df_in) for df_in in pthvalue]))\n",
    "        df_out = df_out.withColumn('ranking', threshold_index_udf(columnname, \"pth\"))\n",
    "        if invert:\n",
    "            df_out = df_out.withColumn(\"maxpth\", lit(x))\n",
    "            df_out = df_out.withColumn(output_name, col(\"maxpth\") - col(\"ranking\")).drop(\"maxpth\")\n",
    "        else:\n",
    "            df_out = df_out.withColumn(output_name, lit(\"ranking\"))\n",
    "        df_out = df_out.drop(\"pth\")\n",
    "        return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6c8cb2-e9ec-4b76-a985-4c756a4e36b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "3ALhQDbI5GgP",
    "outputId": "f94eeab4-7977-4fee-a49f-c2115f37a932"
   },
   "outputs": [],
   "source": [
    "output_data=Discretize.indexer(df_hero_weight,\"Weight\",100,\"Percnetile_weight\")\n",
    "output_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c91c59ed-dd7c-4540-9123-89adc1c1c50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "yUQ9JABUC0DL"
   },
   "source": [
    "## Numerical to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb5b339d-20f5-42e3-a2f8-0d3321ea69b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "lJoS8CMq74j-",
    "outputId": "7798f2b6-e96b-4ee5-83fd-6e6cd0494cb9"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "lookup = spark.createDataFrame(\n",
    "    [(-100.0,0.000,\"NA\"),\n",
    "     (0.001,50.00,\"0-50 Lbs\"),\n",
    "     (50.00,100.00,\"51-100 Lbs\"),\n",
    "     (100.00,200.00,\"101-200 Lbs\"),\n",
    "     (200.00,300.00,\"201-300 Lbs\"),\n",
    "     (300.00,400.00,\"301-400 Lbs\"),\n",
    "     (400.00,500.00,\"401-500 Lbs\"),\n",
    "     (500.00,600.00,\"501-600 Lbs\"),\n",
    "     (600.00,1000.00,\"600+ Lbs\")],\n",
    "    (\"b\",\"t\",\"weight_grp\"))\n",
    "    \n",
    "df_test_grp = df_hero_indi\\\n",
    "    .join(lookup,[F.col(\"weight\")>=F.col(\"b\"),F.col(\"weight\") < F.col(\"t\")],\"leftouter\")\n",
    "  \n",
    "df_test_grp.groupby(\"weight_grp\").count().orderBy(\"weight_grp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201a7ee4-15ad-4d9b-b4a0-a1a3e269cca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "6xjlDLmtSt8O",
    "outputId": "543a2455-6636-4bf4-d9e6-653c5e6e2970"
   },
   "outputs": [],
   "source": [
    "df_test_grp2 = df_test_nc\\\n",
    "    .join(lookup,[F.col(\"clean_weight1\")>=F.col(\"b\"),F.col(\"clean_weight1\") < F.col(\"t\")],\"leftouter\")\n",
    "  \n",
    "df_test_grp2.groupby(\"weight_grp\").count().orderBy(\"weight_grp\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29a49b3e-e5a3-4089-9805-3cfb07986aa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "3K1IuPZd2qhI"
   },
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84badc3f-1c55-46c4-b141-4831f71427fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w4mpJIR320_J"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4776691178928421>, line 4\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLUtils\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n",
       "\u001B[0;32m----> 4\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscaled_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m      5\u001B[0m                         withStd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, withMean\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
       "\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Compute summary statistics by fitting the StandardScaler\u001B[39;00m\n",
       "\u001B[1;32m      8\u001B[0m scalerModel \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit(df_hero_indi)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
       "\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n",
       "\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/feature.py:4389\u001B[0m, in \u001B[0;36mStandardScaler.__init__\u001B[0;34m(self, withMean, withStd, inputCol, outputCol)\u001B[0m\n",
       "\u001B[1;32m   4385\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   4386\u001B[0m \u001B[38;5;124;03m__init__(self, \\\\*, withMean=False, withStd=True, inputCol=None, outputCol=None)\u001B[39;00m\n",
       "\u001B[1;32m   4387\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   4388\u001B[0m \u001B[38;5;28msuper\u001B[39m(StandardScaler, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
       "\u001B[0;32m-> 4389\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.feature.StandardScaler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid)\n",
       "\u001B[1;32m   4390\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n",
       "\u001B[1;32m   4391\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetParams(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/wrapper.py:90\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n",
       "\u001B[1;32m     88\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n",
       "\u001B[1;32m     89\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n",
       "\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_obj(\u001B[38;5;241m*\u001B[39mjava_args)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1620\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1614\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1616\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1617\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1619\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1620\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1621\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fqn)\n",
       "\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
       "\n",
       "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.ml.feature.StandardScaler. Trace:\n",
       "py4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.StandardScaler(java.lang.String) is not whitelisted.\n",
       "\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:256)\n",
       "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
       "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JError",
        "evalue": "An error occurred while calling None.org.apache.spark.ml.feature.StandardScaler. Trace:\npy4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.StandardScaler(java.lang.String) is not whitelisted.\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n\tat py4j.Gateway.invoke(Gateway.java:256)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n"
       },
       "metadata": {
        "errorSummary": ""
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-4776691178928421>, line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MLUtils\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m----> 4\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler(inputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWeight\u001B[39m\u001B[38;5;124m\"\u001B[39m, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscaled_weight\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      5\u001B[0m                         withStd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, withMean\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# Compute summary statistics by fitting the StandardScaler\u001B[39;00m\n\u001B[1;32m      8\u001B[0m scalerModel \u001B[38;5;241m=\u001B[39m scaler\u001B[38;5;241m.\u001B[39mfit(df_hero_indi)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/__init__.py:120\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[0;32m--> 120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/feature.py:4389\u001B[0m, in \u001B[0;36mStandardScaler.__init__\u001B[0;34m(self, withMean, withStd, inputCol, outputCol)\u001B[0m\n\u001B[1;32m   4385\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4386\u001B[0m \u001B[38;5;124;03m__init__(self, \\\\*, withMean=False, withStd=True, inputCol=None, outputCol=None)\u001B[39;00m\n\u001B[1;32m   4387\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4388\u001B[0m \u001B[38;5;28msuper\u001B[39m(StandardScaler, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m-> 4389\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new_java_obj(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.ml.feature.StandardScaler\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muid)\n\u001B[1;32m   4390\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs\n\u001B[1;32m   4391\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetParams(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/ml/wrapper.py:90\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[0;34m(java_class, *args)\u001B[0m\n\u001B[1;32m     88\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[1;32m     89\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[0;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m java_obj(\u001B[38;5;241m*\u001B[39mjava_args)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/java_gateway.py:1620\u001B[0m, in \u001B[0;36mJavaClass.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1614\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCONSTRUCTOR_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1615\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_command_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1616\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1617\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1619\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1620\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1621\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gateway_client, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fqn)\n\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1624\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.8-src.zip/py4j/protocol.py:330\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 330\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[1;32m    333\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    334\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    336\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n",
        "\u001B[0;31mPy4JError\u001B[0m: An error occurred while calling None.org.apache.spark.ml.feature.StandardScaler. Trace:\npy4j.security.Py4JSecurityException: Constructor public org.apache.spark.ml.feature.StandardScaler(java.lang.String) is not whitelisted.\n\tat py4j.security.WhitelistingPy4JSecurityManager.checkConstructor(WhitelistingPy4JSecurityManager.java:451)\n\tat py4j.Gateway.invoke(Gateway.java:256)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:197)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:117)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"Weight\", outputCol=\"scaled_weight\",\n",
    "                        withStd=True, withMean=False)\n",
    "\n",
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(df_hero_indi)\n",
    "\n",
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(df_hero_indi)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4233889-8fe4-4a38-a009-1725130fed8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "eSXErnlI37uO"
   },
   "source": [
    "More reading: https://spark.apache.org/docs/1.4.1/ml-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6feafb-0cc7-4448-9548-d6d2c5d3cd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kfH1jH8jh-P4"
   },
   "source": [
    "#Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76d8e630-1f30-45fc-a9e0-fe8fe29d62f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "1QJAWj9NfL4F"
   },
   "source": [
    "Try to utilize spark as much as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5293e125-6eca-4dea-b0c2-806e11612040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "9sPpi8-LbYR7"
   },
   "source": [
    "## Ingest data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e2d3f18-1ec8-4579-9333-13ed85c147e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "g4Wg3WpmUXpu"
   },
   "source": [
    "Data set: [here](https://www.kaggle.com/mashlyn/online-retail-ii-uci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b2d2d22-8b9a-4d88-a257-053b0e7997be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "6M4nrPN5_Z1W",
    "outputId": "53acb818-6ffa-4e8d-8888-eb11c09a9052"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/My Drive/Kaggle\"\n",
    "\n",
    "#Adding API code here\n",
    "\n",
    "#Unzip and delete zip file\n",
    "!unzip ##  && rm ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a293b05-5c51-4242-9bba-eee6ed7aee85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "nvrbcfIXbSt7"
   },
   "source": [
    "## Data Description\n",
    "\n",
    "This Online Retail II data set contains all the transactions occurring for a UK-based and registered, \n",
    "non-store online retail between 01/12/2009 and 09/12/2011. The company mainly sells unique all-occasion gift-ware. \n",
    "Many customers of the company are wholesalers.\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "- InvoiceNo: Invoice number. Nominal. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.\n",
    "- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "- Description: Product (item) name. Nominal.\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "- InvoiceDate: Invice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- UnitPrice: Unit price. Numeric. Product price per unit in sterling ().\n",
    "- CustomerID: Customer number. Nominal. A 5-digit integral number uniquely assigned to each customer.\n",
    "- Country: Country name. Nominal. The name of the country where a customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc42ed57-220d-4dfe-bb09-2846138eafc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "z1YoN645USbk"
   },
   "source": [
    "### 1. Explore the Data: Check NULL values, Check for outliers, and highlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd67b28b-10f8-4ae1-b1dd-0aef8949b179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "T9aI9lEFUKen"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a38160c-1c85-49e5-a774-03c81b5c5b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "PgXtwt7JdkVu"
   },
   "source": [
    "## For all questions, assume the current date is 10/12/2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c501574c-dd2d-4ddd-bdaa-0b23b926d77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "R9g7xbaTdX4Y"
   },
   "source": [
    "### 2. Find an average basket size of customer in each country in the year 2010\n",
    "\n",
    "#### Basket size = Total Sales Amount / Total Number of Invoices\n",
    "\n",
    "Hint: df.select(to_date(df.STRING_COLUMN).alias('new_date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4161ed7-05bd-4e53-9028-864d94e0affc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "ZuFzPpNjeQnc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fcbaff8-c33f-479b-8b69-4c9e05db683d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "gX8T0Y6aeaW5"
   },
   "source": [
    "###  3. Does the basket size in each country change over time? Which country has the highest growth in terms of both sales amount and basket size in the past 6 months?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e511ebe9-0ddf-42e6-8c14-537cd1944670",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "d1Raj77XegSl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0589380-23ec-4863-9642-14b53f87ff5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "kTvMhqd9ehoy"
   },
   "source": [
    "### 4. Monitor weekly sales and visit by country, Past 1 week, Past 2 weeks, Past 4 weeks, Year-to-date\n",
    "#### Create a report that includes the following columns:\n",
    "- Country\n",
    "- Number of Customers in past 1 week\n",
    "- Number of Customers in past 2 weeks\n",
    "- Number of Customers in past 4 weeks\n",
    "- Number of Customers accumulated since 01/01/2011\n",
    "- Sales amount in past 1 week\n",
    "- Sales amount in past 2 weeks\n",
    "- Sales amount in past 4 weeks\n",
    "- Sales amount since 01/01/2011\n",
    "- Number of Invoices in past 1 week\n",
    "- Number of Invoices in past 2 weeks\n",
    "- Number of Invoices in past 4 weeks\n",
    "- Number of Invoices since 01/01/2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c8e6eb6-40e2-429c-92f1-8fd770f9f88c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "m7auHJ3Deoo9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95062baf-3f8d-4615-9390-52300a88dccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "L_SzPYRadtP3"
   },
   "source": [
    "### 5. Find the average number of days since last visit of the customer in each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d543d56-c1ca-491f-bdd9-ae0f40904222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "w63raV3sfFLo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4776691178928370,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lecture3_ Intro2Spark_Dataframe",
   "widgets": {}
  },
  "colab": {
   "authorship_tag": "ABX9TyPxc/lXh+/fwRHvVHqm5b7d",
   "include_colab_link": true,
   "name": "Intro2Spark-Dataframe.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}